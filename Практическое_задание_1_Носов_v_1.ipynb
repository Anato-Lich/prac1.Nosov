{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "atqZGIIyNSBb"
      },
      "source": [
        "#**Практическое задание №1**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ga5g3lUhNNBy"
      },
      "source": [
        "Установка необходимых пакетов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TGBk36LpukIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8ccc650-6579-4929-e837-a3aebc737a76"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.8/dist-packages (4.4.0)\n",
            "Collecting gdown\n",
            "  Downloading gdown-4.5.4-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from gdown) (3.8.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.8/dist-packages (from gdown) (4.6.3)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.8/dist-packages (from gdown) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from gdown) (4.64.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from gdown) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (2022.9.24)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.8/dist-packages (from requests[socks]->gdown) (1.7.1)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 4.4.0\n",
            "    Uninstalling gdown-4.4.0:\n",
            "      Successfully uninstalled gdown-4.4.0\n",
            "Successfully installed gdown-4.5.4\n"
          ]
        }
      ],
      "source": [
        "!pip install -q tqdm\n",
        "!pip install --upgrade --no-cache-dir gdown"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vQDLyHEO1Ux"
      },
      "source": [
        "Монтирование Вашего Google Drive к текущему окружению:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5G5KkA1Nu5M9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a05a9f54-7721-4370-95d1-66ac0b6d8321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Num5lHV6912"
      },
      "source": [
        "Константы, которые пригодятся в коде далее, и ссылки (gdrive идентификаторы) на предоставляемые наборы данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ab2yCwDm7Fqb"
      },
      "outputs": [],
      "source": [
        "EVALUATE_ONLY = True\n",
        "TEST_ON_LARGE_DATASET = True\n",
        "TISSUE_CLASSES = ('ADI', 'BACK', 'DEB', 'LYM', 'MUC', 'MUS', 'NORM', 'STR', 'TUM')\n",
        "DATASETS_LINKS = {\n",
        "    'train': '1XtQzVQ5XbrfxpLHJuL0XBGJ5U7CS-cLi',\n",
        "    'train_small': '1qd45xXfDwdZjktLFwQb-et-mAaFeCzOR',\n",
        "    'train_tiny': '1I-2ZOuXLd4QwhZQQltp817Kn3J0Xgbui',\n",
        "    'test': '1RfPou3pFKpuHDJZ-D9XDFzgvwpUBFlDr',\n",
        "    'test_small': '1wbRsog0n7uGlHIPGLhyN-PMeT2kdQ2lI',\n",
        "    'test_tiny': '1viiB0s041CNsAK4itvX8PnYthJ-MDnQc'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgY-ux5qOI0k"
      },
      "source": [
        "Импорт необходимых зависимостей:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLHQhqiSIyvK"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from typing import List\n",
        "from tqdm.notebook import tqdm\n",
        "from time import sleep\n",
        "from PIL import Image\n",
        "import IPython.display\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "import gdown\n",
        "\n",
        "\n",
        "#LBL1 - дополнительные библиотеки\n",
        "from torchvision.models import resnet50, ResNet\n",
        "import torch, torchvision\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKLI3lUyMYO9"
      },
      "source": [
        "---\n",
        "### Класс Dataset\n",
        "\n",
        "Предназначен для работы с наборами данных, обеспечивает чтение изображений и соответствующих меток, а также формирование пакетов (батчей)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "8N169efsw1ej"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.is_loaded = False\n",
        "        #LBL2 - изменена загрузка датасета. Файлы должны быть в корне диска\n",
        "        url = f'https://drive.google.com/uc?id={DATASETS_LINKS[name]}'\n",
        "        output = f'{name}.npz'\n",
        "        gdown.download(url, output, quiet=False)\n",
        "        print(f'Loading dataset {self.name} from npz.')\n",
        "        np_obj = np.load(f'/content/drive/MyDrive/{name}.npz')\n",
        "        self.images = np_obj['data']\n",
        "        self.labels = np_obj['labels']\n",
        "        self.n_files = self.images.shape[0]\n",
        "        self.is_loaded = True\n",
        "        \n",
        "        self.transform = torchvision.transforms.Normalize(0, 1)\n",
        "\n",
        "        print(f'Done. Dataset {name} consists of {self.n_files} images.')\n",
        "\n",
        "    def image(self, i):\n",
        "        # read i-th image in dataset and return it as numpy array\n",
        "        if self.is_loaded:\n",
        "            return self.images[i, :, :, :]\n",
        "\n",
        "    def images_seq(self, n=None):\n",
        "        # sequential access to images inside dataset (is needed for testing)\n",
        "        for i in range(self.n_files if not n else n):\n",
        "            yield self.image(i)\n",
        "\n",
        "    def random_image_with_label(self):\n",
        "        # get random image with label from dataset\n",
        "        i = np.random.randint(self.n_files)\n",
        "        return self.image(i), self.labels[i]\n",
        "  \n",
        "    def random_batch_with_labels(self, n):\n",
        "        # create random batch of images with labels (is needed for training)\n",
        "        indices = np.random.choice(self.n_files, n)\n",
        "        imgs = []\n",
        "        for i in indices:\n",
        "            img = self.image(i)\n",
        "            imgs.append(self.image(i))\n",
        "        logits = np.array([self.labels[i] for i in indices])\n",
        "        return np.stack(imgs), logits\n",
        "\n",
        "    def image_with_label(self, i: int):\n",
        "        # return i-th image with label from dataset\n",
        "        return self.image(i), self.labels[i]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(\n",
        "        self,\n",
        "        idx: int,\n",
        "    ) -> Tuple[torch.FloatTensor, int]:\n",
        "        #LBL3 - реализация getitem.\n",
        "        img = torch.Tensor(self.images[idx]).view(3, 224, 224)\n",
        "        img = self.transform(img)\n",
        "        return torch.FloatTensor(img), self.labels[idx]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M-LvGqeHYgus"
      },
      "source": [
        "### Пример использвания класса Dataset\n",
        "Загрузим обучающий набор данных, получим произвольное изображение с меткой. После чего визуализируем изображение, выведем метку. В будущем, этот кусок кода можно закомментировать или убрать."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "HhObWEjGJ1um",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "d4781d49-1716-42e2-bb58-dfd772e2a696"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1I-2ZOuXLd4QwhZQQltp817Kn3J0Xgbui\n",
            "To: /content/train_tiny.npz\n",
            "100%|██████████| 105M/105M [00:03<00:00, 33.9MB/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset train_tiny from npz.\n",
            "Done. Dataset train_tiny consists of 900 images.\n",
            "\n",
            "Got numpy array of shape (224, 224, 3), and label with code 1.\n",
            "Label code corresponds to BACK class.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=224x224 at 0x7FCF5E2012B0>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "d_train_tiny = Dataset('train_tiny')\n",
        "\n",
        "img, lbl = d_train_tiny.random_image_with_label()\n",
        "print()\n",
        "print(f'Got numpy array of shape {img.shape}, and label with code {lbl}.')\n",
        "print(f'Label code corresponds to {TISSUE_CLASSES[lbl]} class.')\n",
        "\n",
        "pil_img = Image.fromarray(img)\n",
        "IPython.display.display(pil_img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaBXXCWeVLYb"
      },
      "source": [
        "---\n",
        "### Класс Metrics\n",
        "\n",
        "Реализует метрики точности, используемые для оценивания модели:\n",
        "1. точность,\n",
        "2. сбалансированную точность."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5unQ7azTinCZ"
      },
      "outputs": [],
      "source": [
        "class Metrics:\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy(gt: List[int], pred: List[int]):\n",
        "        assert len(gt) == len(pred), 'gt and prediction should be of equal length'\n",
        "        # print(type(gt), type(pred))\n",
        "        # print( tuple(zip(gt, pred))[1] )\n",
        "        return sum(int(i[0] == i[1]) for i in zip(gt, pred)) / len(gt)\n",
        "\n",
        "    @staticmethod\n",
        "    def accuracy_balanced(gt: List[int], pred: List[int]):\n",
        "        return balanced_accuracy_score(gt, pred)\n",
        "\n",
        "    @staticmethod\n",
        "    def print_all(gt: List[int], pred: List[int], info: str):\n",
        "        print(f'metrics for {info}:')\n",
        "        print('\\t accuracy {:.4f}:'.format(Metrics.accuracy(gt, pred)))\n",
        "        print('\\t balanced accuracy {:.4f}:'.format(Metrics.accuracy_balanced(gt, pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1AHzTJVGU5k"
      },
      "source": [
        "---\n",
        "### Класс Model\n",
        "\n",
        "Класс, хранящий в себе всю информацию о модели.\n",
        "\n",
        "Вам необходимо реализовать методы save, load для сохранения и заргрузки модели. Особенно актуально это будет во время тестирования на дополнительных наборах данных.\n",
        "\n",
        "> *Пожалуйста, убедитесь, что сохранение и загрузка модели работает корректно. Для этого обучите модель, протестируйте, сохраните ее в файл, перезапустите среду выполнения, загрузите обученную модель из файла, вновь протестируйте ее на тестовой выборке и убедитесь в том, что получаемые метрики совпадают с полученными для тестовой выбрки ранее.*\n",
        "\n",
        "\n",
        "Также, Вы можете реализовать дополнительные функции, такие как:\n",
        "1. валидацию модели на части обучающей выборки;\n",
        "2. использование кроссвалидации;\n",
        "3. автоматическое сохранение модели при обучении;\n",
        "4. загрузку модели с какой-то конкретной итерации обучения (если используется итеративное обучение);\n",
        "5. вывод различных показателей в процессе обучения (например, значение функции потерь на каждой эпохе);\n",
        "6. построение графиков, визуализирующих процесс обучения (например, график зависимости функции потерь от номера эпохи обучения);\n",
        "7. автоматическое тестирование на тестовом наборе/наборах данных после каждой эпохи обучения (при использовании итеративного обучения);\n",
        "8. автоматический выбор гиперпараметров модели во время обучения;\n",
        "9. сохранение и визуализацию результатов тестирования;\n",
        "10. Использование аугментации и других способов синтетического расширения набора данных (дополнительным плюсом будет обоснование необходимости и обоснование выбора конкретных типов аугментации)\n",
        "11. и т.д.\n",
        "\n",
        "Полный список опций и дополнений приведен в презентации с описанием задания.\n",
        "\n",
        "При реализации дополнительных функций допускается добавление параметров в существующие методы и добавление новых методов в класс модели."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "shfhhu4w030m"
      },
      "outputs": [],
      "source": [
        "#LBL4 - реализация метрик для подсчета по эпохам и вывода в тензорборд.\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def compute_metrics(\n",
        "    outputs: torch.Tensor,\n",
        "    labels: torch.LongTensor,\n",
        ") -> Dict[str, float]:\n",
        "\n",
        "    metrics = {}\n",
        "\n",
        "    _, pred = torch.max(outputs.data, 1)\n",
        "    y_true = labels.cpu()\n",
        "    y_pred = pred.cpu()\n",
        "\n",
        "    # precision\n",
        "    precision_micro = precision_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"micro\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "    precision_macro = precision_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"macro\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "    precision_weighted = precision_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"weighted\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "\n",
        "\n",
        "    # f1\n",
        "    f1_micro = f1_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"micro\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "    f1_macro = f1_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"macro\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "    f1_weighted = f1_score(\n",
        "        y_true=y_true,\n",
        "        y_pred=y_pred,\n",
        "        average=\"weighted\",\n",
        "        zero_division=0,\n",
        "    )\n",
        "\n",
        "\n",
        "    metrics[\"precision_micro\"]    = precision_micro\n",
        "    metrics[\"precision_macro\"]    = precision_macro\n",
        "    metrics[\"precision_weighted\"] = precision_weighted\n",
        "\n",
        "    metrics[\"f1_micro\"]    = f1_micro\n",
        "    metrics[\"f1_macro\"]    = f1_macro\n",
        "    metrics[\"f1_weighted\"] = f1_weighted\n",
        "\n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Cywp17Y5030m"
      },
      "outputs": [],
      "source": [
        "#LBL5 - реализация эпохи обучения\n",
        "from collections import defaultdict\n",
        "\n",
        "def train_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    One training cycle (loop).\n",
        "    \"\"\"\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "\n",
        "    for i, (images, labels) in tqdm(\n",
        "        enumerate(dataloader),\n",
        "        total=len(dataloader),\n",
        "        desc=\"loop over train batches\",\n",
        "    ):\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Подсчет лосса и шаг оптимизатора\n",
        "        optimizer.zero_grad()\n",
        "        output = model(images)\n",
        "        loss = criterion(output, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss.append(loss.item())\n",
        "        writer.add_scalar(\n",
        "            \"batch loss / train\", loss.item(), epoch * len(dataloader) + i\n",
        "        )\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            outputs_inference = model(images)\n",
        "            model.train()\n",
        "\n",
        "        batch_metrics = compute_metrics(\n",
        "            outputs=outputs_inference,\n",
        "            labels=labels,\n",
        "        )\n",
        "\n",
        "        for metric_name, metric_value in batch_metrics.items():\n",
        "            batch_metrics_list[metric_name].append(metric_value)\n",
        "            writer.add_scalar(\n",
        "                f\"batch {metric_name} / train\",\n",
        "                metric_value,\n",
        "                epoch * len(dataloader) + i,\n",
        "            )\n",
        "\n",
        "    avg_loss = np.mean(epoch_loss)\n",
        "    print(f\"Train loss: {avg_loss}\\n\")\n",
        "    writer.add_scalar(\"loss / train\", avg_loss, epoch)\n",
        "\n",
        "    for metric_name, metric_value_list in batch_metrics_list.items():\n",
        "        metric_value = np.mean(metric_value_list)\n",
        "        print(f\"Train {metric_name}: {metric_value}\\n\")\n",
        "        writer.add_scalar(f\"{metric_name} / train\", metric_value, epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OlCaJhdZ030m"
      },
      "outputs": [],
      "source": [
        "#LBL6 - реализация эпохи валидации на тестовом датасете.\n",
        "def evaluate_epoch(\n",
        "    model: torch.nn.Module,\n",
        "    dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        "    epoch: int,\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    One evaluation cycle (loop).\n",
        "    \"\"\"\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = []\n",
        "    batch_metrics_list = defaultdict(list)\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for i, (tokens, labels) in tqdm(\n",
        "            enumerate(dataloader),\n",
        "            total=len(dataloader),\n",
        "            desc=\"loop over test batches\",\n",
        "        ):\n",
        "\n",
        "            tokens, labels = tokens.to(device), labels.to(device)\n",
        "\n",
        "            # Подсчет лосса\n",
        "            \n",
        "            outputs = model(tokens)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            epoch_loss.append(loss.item())\n",
        "            writer.add_scalar(\n",
        "                \"batch loss / test\", loss.item(), epoch * len(dataloader) + i\n",
        "            )\n",
        "\n",
        "            batch_metrics = compute_metrics(\n",
        "                outputs=outputs,\n",
        "                labels=labels,\n",
        "            )\n",
        "\n",
        "            for metric_name, metric_value in batch_metrics.items():\n",
        "                batch_metrics_list[metric_name].append(metric_value)\n",
        "                writer.add_scalar(\n",
        "                    f\"batch {metric_name} / test\",\n",
        "                    metric_value,\n",
        "                    epoch * len(dataloader) + i,\n",
        "                )\n",
        "\n",
        "        avg_loss = np.mean(epoch_loss)\n",
        "        print(f\"Test loss:  {avg_loss}\\n\")\n",
        "        writer.add_scalar(\"loss / test\", avg_loss, epoch)\n",
        "\n",
        "        for metric_name, metric_value_list in batch_metrics_list.items():\n",
        "            metric_value = np.mean(metric_value_list)\n",
        "            print(f\"Test {metric_name}: {metric_value}\\n\")\n",
        "            writer.add_scalar(f\"{metric_name} / test\", np.mean(metric_value), epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "0pkMiB6mJ7JQ"
      },
      "outputs": [],
      "source": [
        "class Model:\n",
        "\n",
        "    def __init__(self):\n",
        "        # todo\n",
        "        super().__init__()\n",
        "\n",
        "        self.features = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "        self.features.fc = torch.nn.Linear(512, 9)\n",
        "        \n",
        "        # print(self.features)\n",
        "        self.features.eval()\n",
        "\n",
        "\n",
        "    def save(self, name: str):\n",
        "        # todo\n",
        "        # pass\n",
        "        # example demonstrating saving the model to PROJECT_DIR folder on gdrive with name 'name'\n",
        "\n",
        "        torch.save(self.features.state_dict(), f'/content/drive/MyDrive/{name}.pth')\n",
        "\n",
        "        # arr = np.array([1, 2, 3, 4, 5], dtype=np.float32)\n",
        "        # np.savez(f'/content/drive/MyDrive/{name}.npz', data=arr)\n",
        "\n",
        "    def load(self, name: str):\n",
        "        # todo\n",
        "        pass\n",
        "        # example demonstrating loading the model with name 'name' from gdrive using link\n",
        "        name_to_id_dict = {\n",
        "            'best': '1-491lAqNlqD-_Fk5XMyXVLHb7KdzIxbQ'\n",
        "        }\n",
        "        # https://drive.google.com/file/d/1-491lAqNlqD-_Fk5XMyXVLHb7KdzIxbQ/view?usp=share_link\n",
        "        #LBL7 - загрузка весов модели. Веса должны быть в корне диска.\n",
        "        output = name\n",
        "        # output = f'/content/drive/MyDrive/{name}.pth'\n",
        "        gdown.download(f'https://drive.google.com/uc?id={name_to_id_dict[name]}', output, quiet=False)\n",
        "\n",
        "        self.features.load_state_dict(torch.load(output))\n",
        "        print(self.features)\n",
        "\n",
        "\n",
        "    #LBL8 - реализация обучения модели с сохранением промежуточных весов в \n",
        "    # median_weights\n",
        "    def train(self,\n",
        "    n_epochs: int,\n",
        "    train_dataloader: torch.utils.data.DataLoader,\n",
        "    test_dataloader: torch.utils.data.DataLoader,\n",
        "    criterion: torch.nn.Module,\n",
        "    writer: SummaryWriter,\n",
        "    device: torch.device,\n",
        "    ) -> None:\n",
        "            \n",
        "        optimizer = torch.optim.Adam(self.features.parameters())\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "\n",
        "            print(f\"Epoch [{epoch+1} / {n_epochs}]\\n\")\n",
        "\n",
        "            train_epoch(\n",
        "                model=self.features,\n",
        "                dataloader=train_dataloader,\n",
        "                optimizer=optimizer,\n",
        "                criterion=criterion,\n",
        "                writer=writer,\n",
        "                device=device,\n",
        "                epoch=epoch,\n",
        "            )\n",
        "            evaluate_epoch(\n",
        "                model=self.features,\n",
        "                dataloader=test_dataloader,\n",
        "                criterion=criterion,\n",
        "                writer=writer,\n",
        "                device=device,\n",
        "                epoch=epoch,\n",
        "            )\n",
        "            \n",
        "            self.save('median_weights')\n",
        "                \n",
        "        print(f'training done')\n",
        "\n",
        "\n",
        "    def test_on_dataset(self, dataset: Dataset, limit=None):\n",
        "        # you can upgrade this code if you want to speed up testing using batches\n",
        "        predictions = []\n",
        "        n = dataset.n_files if not limit else int(dataset.n_files * limit)\n",
        "        for img in tqdm(dataset.images_seq(n), total=n):\n",
        "            predictions.append(self.test_on_image(img))\n",
        "        return predictions\n",
        "\n",
        "    def test_on_image(self, img: np.ndarray):\n",
        "        # todo: replace this code\n",
        "        img = torch.FloatTensor(img).cuda()\n",
        "        transform = torchvision.transforms.Normalize(0, 1)\n",
        "\n",
        "        # print(transform(img).size())\n",
        "        # a = torch.Tensor([1]).cuda()\n",
        "        # pred = torch.cat((a, transform(img).view(3, 224, 224)), 0).cuda()\n",
        "        prediction = self.features(transform(img).view(1, 3, 224, 224))\n",
        "        del img\n",
        "        torch.cuda.empty_cache()\n",
        "        return prediction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMpTB6lMr00A"
      },
      "source": [
        "---\n",
        "### Классификация изображений\n",
        "\n",
        "Используя введенные выше классы можем перейти уже непосредственно к обучению модели классификации изображений. Пример общего пайплайна решения задачи приведен ниже. Вы можете его расширять и улучшать. В данном примере используются наборы данных 'train_small' и 'test_small'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "5cTOuZD01Up6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dce6dd50-9a6a-4a82-ca9e-754cfc5bfda9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1XtQzVQ5XbrfxpLHJuL0XBGJ5U7CS-cLi\n",
            "To: /content/train.npz\n",
            "100%|██████████| 2.10G/2.10G [00:49<00:00, 42.1MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset train from npz.\n",
            "Done. Dataset train consists of 18000 images.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1RfPou3pFKpuHDJZ-D9XDFzgvwpUBFlDr\n",
            "To: /content/test.npz\n",
            "100%|██████████| 525M/525M [00:15<00:00, 34.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset test from npz.\n",
            "Done. Dataset test consists of 4500 images.\n"
          ]
        }
      ],
      "source": [
        "d_train = Dataset('train')\n",
        "d_test = Dataset('test')\n",
        "#LBL9 - создание даталоадера для работы с ResNet18\n",
        "train_dataloader = torch.utils.data.DataLoader(d_train, batch_size = 64,\n",
        "                                          shuffle=True, num_workers = 2)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(d_test, batch_size = 64,\n",
        "                                          shuffle=False, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataiter = iter(test_dataloader)\n",
        "images, labels = dataiter.next()\n",
        "print(type(images))\n",
        "print(images.shape)\n",
        "print(labels.shape)"
      ],
      "metadata": {
        "id": "8GgxsTBeQwbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LBL10 - вывод слоев в модели.\n",
        "from torchsummary import summary\n",
        "\n",
        "model = Model()\n",
        "model.features.cuda()\n",
        "summary(model.features, (3, 224, 224), -1)"
      ],
      "metadata": {
        "id": "rbCrlqpeCXXr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LBL11 - тензорборд с метриками\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "metadata": {
        "id": "c6Z_AVZ7VQ2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wBi0XpXg8_wq"
      },
      "outputs": [],
      "source": [
        "device = 'cuda'\n",
        "\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss() #torch.Tensor([1, 1.3, 1, 1, 1, 1, 1, 1, 1]).to(device))\n",
        "writer = SummaryWriter('logs/model')\n",
        "\n",
        "model.features.to(device)\n",
        "if not EVALUATE_ONLY:\n",
        "    model.train(\n",
        "    10,\n",
        "    train_dataloader,\n",
        "    test_dataloader,\n",
        "    criterion,\n",
        "    writer,\n",
        "    device)\n",
        "    \n",
        "    model.save('best')\n",
        "else:\n",
        "    # todo: your link goes here\n",
        "    model.load('best')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcM2EiRMVP93"
      },
      "source": [
        "Пример тестирования модели на части набора данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0AqmeLEKqrs"
      },
      "outputs": [],
      "source": [
        "# evaluating model on 10% of test dataset\n",
        "with torch.no_grad():\n",
        "    model.features.eval()\n",
        "    pred_1 = model.test_on_dataset(d_test, limit=0.1)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(pred_1[1])"
      ],
      "metadata": {
        "id": "A0XM0QdYKBTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_1_test = torch.cat(pred_1, 0)\n",
        "print(pred_1_test.size())"
      ],
      "metadata": {
        "id": "MBSEpdr7NhTi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, pred = torch.max(pred_1_test, 1)\n",
        "# print(pred[1])\n",
        "Metrics.print_all(d_test.labels[:len(pred_1)].tolist(), pred.tolist(), '10% of test')"
      ],
      "metadata": {
        "id": "4ov9bX4uJtup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "XI6AMoTBRyv3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSwvHVVzVWZ5"
      },
      "source": [
        "Пример тестирования модели на полном наборе данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjI_sbMi3TMY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "15816baa917449adab89fb07c21f87e7",
            "d0331656f634461ebca86d9e1afc8a40",
            "7e3fa845435143d88a8e2ebd970e26b6",
            "12f2ff1faabe4b8f8911b58836494ccf",
            "feb4f34181e943b8b964bc747859e8cc",
            "17e3e8962808497fb926bbbaedb3e770",
            "d20b7752b81d4b6a8583920760ed8a1d",
            "a93b1ec91f474b9f89f7721cc3ec7cac",
            "c114ae6f541e4a9793cf29b6d5abb05a",
            "8259e21d75da4b23bdd325fb53ef2daf",
            "bc37b4fd14b140e1bc03bcbfa372a535"
          ]
        },
        "outputId": "dbf2a6ac-b369-4ea2-e54e-608329212f8e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/4500 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15816baa917449adab89fb07c21f87e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metrics for test:\n",
            "\t accuracy 0.9147:\n",
            "\t balanced accuracy 0.9147:\n"
          ]
        }
      ],
      "source": [
        "# evaluating model on full test dataset (may take time)\n",
        "with torch.no_grad():\n",
        "    model.features.eval()\n",
        "    if TEST_ON_LARGE_DATASET:\n",
        "        pred_2 = model.test_on_dataset(d_test)\n",
        "        pred_2_test = torch.cat(pred_2, 0)\n",
        "        _, pred = torch.max(pred_2_test, 1)\n",
        "        Metrics.print_all(d_test.labels, pred.tolist(), 'test')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvyEHdxEB18o"
      },
      "source": [
        "Результат работы пайплайна обучения и тестирования выше тоже будет оцениваться. Поэтому не забудьте присылать на проверку ноутбук с выполнеными ячейками кода с демонстрациями метрик обучения, графиками и т.п. В этом пайплайне Вам необходимо продемонстрировать работу всех реализованных дополнений, улучшений и т.п.\n",
        "\n",
        "<font color=\"red\">\n",
        "Настоятельно рекомендуется после получения пайплайна с полными результатами обучения экспортировать ноутбук в pdf (файл -> печать) и прислать этот pdf вместе с самим ноутбуком.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzSKAvVI6uCW"
      },
      "source": [
        "### Тестирование модели на других наборах данных\n",
        "\n",
        "Ваша модель должна поддерживать тестирование на других наборах данных. Для удобства, Вам предоставляется набор данных test_tiny, который представляет собой малую часть (2% изображений) набора test. Ниже приведен фрагмент кода, который будет осуществлять тестирование для оценивания Вашей модели на дополнительных тестовых наборах данных.\n",
        "\n",
        "<font color=\"red\">\n",
        "Прежде чем отсылать задание на проверку, убедитесь в работоспособности фрагмента кода ниже.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "sdY3uTt87tqv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2f94d8f85465481c988049e377e0ac13",
            "3f08f09d33f04457b3d56f4d71a261ba",
            "c3f8fad1c317420890fbb6e6bbbb263b",
            "22e51226d4a2426eb5b36c63b0b31bb1",
            "437a5fc962a045869987341f37b4f680",
            "5d9d73a5dcbd48c4aa1c9b2e5bdbf99f",
            "d7d8bec96dc942089c726cb6a72ffc58",
            "a8c39009b9ae46058c2f4caa142fd449",
            "24e4bac929c745c09b205aee2b1a51ae",
            "004c3e6d76134a1ba1621ee63d4efaf2",
            "45dd2ae0954149989e92600fc45c4434"
          ]
        },
        "outputId": "3760b1a0-3bfb-49a3-d77a-e88630221c79"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.8/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1-491lAqNlqD-_Fk5XMyXVLHb7KdzIxbQ\n",
            "To: /content/best\n",
            "100%|██████████| 44.8M/44.8M [00:00<00:00, 247MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ResNet(\n",
            "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU(inplace=True)\n",
            "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "  (layer1): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer2): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer3): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (layer4): Sequential(\n",
            "    (0): BasicBlock(\n",
            "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (downsample): Sequential(\n",
            "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      )\n",
            "    )\n",
            "    (1): BasicBlock(\n",
            "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "  (fc): Linear(in_features=512, out_features=9, bias=True)\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1viiB0s041CNsAK4itvX8PnYthJ-MDnQc\n",
            "To: /content/test_tiny.npz\n",
            "100%|██████████| 10.6M/10.6M [00:00<00:00, 16.9MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading dataset test_tiny from npz.\n",
            "Done. Dataset test_tiny consists of 90 images.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/90 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f94d8f85465481c988049e377e0ac13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metrics for test-tiny:\n",
            "\t accuracy 0.8667:\n",
            "\t balanced accuracy 0.8667:\n"
          ]
        }
      ],
      "source": [
        "final_model = Model()\n",
        "final_model.load('best')\n",
        "d_test_tiny = Dataset('test_tiny')\n",
        "pred = model.test_on_dataset(d_test_tiny)\n",
        "pred_test = torch.cat(pred, 0)\n",
        "_, pred = torch.max(pred_test, 1)\n",
        "\n",
        "Metrics.print_all(d_test_tiny.labels, pred.tolist(), 'test-tiny')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPvyj4gscU10"
      },
      "source": [
        "Отмонтировать Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfX35zNSvFWn"
      },
      "outputs": [],
      "source": [
        "drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMyDxCDCspcI"
      },
      "source": [
        "---\n",
        "# Дополнительные \"полезности\"\n",
        "\n",
        "Ниже приведены примеры использования различных функций и библиотек, которые могут быть полезны при выполнении данного практического задания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvLwSttCs1rB"
      },
      "source": [
        "### Измерение времени работы кода\n",
        "\n",
        "Измерять время работы какой-либо функции можно легко и непринужденно при помощи функции timeit из соответствующего модуля:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HnLVhwE9C9S"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "\n",
        "def factorial(n):\n",
        "    res = 1\n",
        "    for i in range(1, n + 1):\n",
        "        res *= i\n",
        "    return res\n",
        "\n",
        "\n",
        "def f():\n",
        "    return factorial(n=1000)\n",
        "\n",
        "n_runs = 128\n",
        "print(f'Function f is caluclated {n_runs} times in {timeit.timeit(f, number=n_runs)}s.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fibGVEdguOOi"
      },
      "source": [
        "### Scikit-learn\n",
        "\n",
        "Для использования \"классических\" алгоритмов машинного обучения рекомендуется использовать библиотеку scikit-learn (https://scikit-learn.org/stable/). Пример классификации изображений цифр из набора данных MNIST при помощи классификатора SVM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXHnBzEfunAO"
      },
      "outputs": [],
      "source": [
        "# Standard scientific Python imports\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Import datasets, classifiers and performance metrics\n",
        "from sklearn import datasets, svm, metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# The digits dataset\n",
        "digits = datasets.load_digits()\n",
        "\n",
        "# The data that we are interested in is made of 8x8 images of digits, let's\n",
        "# have a look at the first 4 images, stored in the `images` attribute of the\n",
        "# dataset.  If we were working from image files, we could load them using\n",
        "# matplotlib.pyplot.imread.  Note that each image must have the same size. For these\n",
        "# images, we know which digit they represent: it is given in the 'target' of\n",
        "# the dataset.\n",
        "_, axes = plt.subplots(2, 4)\n",
        "images_and_labels = list(zip(digits.images, digits.target))\n",
        "for ax, (image, label) in zip(axes[0, :], images_and_labels[:4]):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    ax.set_title('Training: %i' % label)\n",
        "\n",
        "# To apply a classifier on this data, we need to flatten the image, to\n",
        "# turn the data in a (samples, feature) matrix:\n",
        "n_samples = len(digits.images)\n",
        "data = digits.images.reshape((n_samples, -1))\n",
        "\n",
        "# Create a classifier: a support vector classifier\n",
        "classifier = svm.SVC(gamma=0.001)\n",
        "\n",
        "# Split data into train and test subsets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data, digits.target, test_size=0.5, shuffle=False)\n",
        "\n",
        "# We learn the digits on the first half of the digits\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Now predict the value of the digit on the second half:\n",
        "predicted = classifier.predict(X_test)\n",
        "\n",
        "images_and_predictions = list(zip(digits.images[n_samples // 2:], predicted))\n",
        "for ax, (image, prediction) in zip(axes[1, :], images_and_predictions[:4]):\n",
        "    ax.set_axis_off()\n",
        "    ax.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    ax.set_title('Prediction: %i' % prediction)\n",
        "\n",
        "print(\"Classification report for classifier %s:\\n%s\\n\"\n",
        "      % (classifier, metrics.classification_report(y_test, predicted)))\n",
        "disp = metrics.plot_confusion_matrix(classifier, X_test, y_test)\n",
        "disp.figure_.suptitle(\"Confusion Matrix\")\n",
        "print(\"Confusion matrix:\\n%s\" % disp.confusion_matrix)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uu3Dny5zxcVy"
      },
      "source": [
        "### Scikit-image\n",
        "\n",
        "Реализовывать различные операции для работы с изображениями можно как самостоятельно, работая с массивами numpy, так и используя специализированные библиотеки, например, scikit-image (https://scikit-image.org/). Ниже приведен пример использования Canny edge detector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5TZvy_d7xc0B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import ndimage as ndi\n",
        "\n",
        "from skimage import feature\n",
        "\n",
        "\n",
        "# Generate noisy image of a square\n",
        "im = np.zeros((128, 128))\n",
        "im[32:-32, 32:-32] = 1\n",
        "\n",
        "im = ndi.rotate(im, 15, mode='constant')\n",
        "im = ndi.gaussian_filter(im, 4)\n",
        "im += 0.2 * np.random.random(im.shape)\n",
        "\n",
        "# Compute the Canny filter for two values of sigma\n",
        "edges1 = feature.canny(im)\n",
        "edges2 = feature.canny(im, sigma=3)\n",
        "\n",
        "# display results\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(nrows=1, ncols=3, figsize=(8, 3),\n",
        "                                    sharex=True, sharey=True)\n",
        "\n",
        "ax1.imshow(im, cmap=plt.cm.gray)\n",
        "ax1.axis('off')\n",
        "ax1.set_title('noisy image', fontsize=20)\n",
        "\n",
        "ax2.imshow(edges1, cmap=plt.cm.gray)\n",
        "ax2.axis('off')\n",
        "ax2.set_title(r'Canny filter, $\\sigma=1$', fontsize=20)\n",
        "\n",
        "ax3.imshow(edges2, cmap=plt.cm.gray)\n",
        "ax3.axis('off')\n",
        "ax3.set_title(r'Canny filter, $\\sigma=3$', fontsize=20)\n",
        "\n",
        "fig.tight_layout()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiEWhGUQRGoH"
      },
      "source": [
        "### Tensorflow 2\n",
        "\n",
        "Для создания и обучения нейросетевых моделей можно использовать фреймворк глубокого обучения Tensorflow 2. Ниже приведен пример простейшей нейроной сети, использующейся для классификации изображений из набора данных MNIST."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDwLG7A1ReNy"
      },
      "outputs": [],
      "source": [
        "# Install TensorFlow\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)\n",
        "\n",
        "model.evaluate(x_test,  y_test, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbvktmLwRu8g"
      },
      "source": [
        "<font color=\"red\">\n",
        "Для эффективной работы с моделями глубокого обучения убедитесь в том, что в текущей среде Google Colab используется аппаратный ускоритель GPU или TPU. Для смены среды выберите \"среда выполнения\" -> \"сменить среду выполнения\".\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJVNOOU9Sjyf"
      },
      "source": [
        "Большое количество туториалов и примеров с кодом на Tensorflow 2 можно найти на официальном сайте https://www.tensorflow.org/tutorials?hl=ru. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVPs3pYpS0U1"
      },
      "source": [
        "Также, Вам может понадобиться написать собственный генератор данных для Tensorflow 2. Скорее всего он будет достаточно простым, и его легко можно будет реализовать, используя официальную документацию TensorFlow 2. Но, на всякий случай (если не удлось сразу разобраться или хочется вникнуть в тему более глубоко), можете посмотреть следующий отличный туториал: https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwI-T0IXyN84"
      },
      "source": [
        "### Numba\n",
        "\n",
        "В некоторых ситуациях, при ручных реализациях графовых алгоритмов, выполнение многократных вложенных циклов for в python можно существенно ускорить, используя JIT-компилятор Numba (https://numba.pydata.org/).\n",
        "Примеры использования Numba в Google Colab можно найти тут:\n",
        "1. https://colab.research.google.com/github/cbernet/maldives/blob/master/numba/numba_cuda.ipynb\n",
        "2. https://colab.research.google.com/github/evaneschneider/parallel-programming/blob/master/COMPASS_gpu_intro.ipynb \n",
        "\n",
        "> Пожалуйста, если Вы решили использовать Numba для решения этого практического задания, еще раз подумайте, нужно ли это Вам, и есть ли возможность реализовать требуемую функциональность иным способом. Используйте Numba только при реальной необходимости.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxAJ00A76LcF"
      },
      "source": [
        "### Работа с zip архивами в Google Drive\n",
        "\n",
        "Запаковка и распаковка zip архивов может пригодиться при сохранении и загрузки Вашей модели. Ниже приведен фрагмент кода, иллюстрирующий помещение нескольких файлов в zip архив с последующим чтением файлов из него. Все действия с директориями, файлами и архивами должны осущетвляться с примонтированным Google Drive.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJiKndOpPu_e"
      },
      "source": [
        "Создадим 2 изображения, поместим их в директорию tmp внутри PROJECT_DIR, запакуем директорию tmp в архив tmp.zip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CRwgPtv-6nMP"
      },
      "outputs": [],
      "source": [
        "PROJECT_DIR = \"/dev/prak_nn_1/\"\n",
        "arr1 = np.random.rand(100, 100, 3) * 255\n",
        "arr2 = np.random.rand(100, 100, 3) * 255\n",
        "\n",
        "img1 = Image.fromarray(arr1.astype('uint8'))\n",
        "img2 = Image.fromarray(arr2.astype('uint8'))\n",
        "\n",
        "p = \"/content/drive/MyDrive/\" + PROJECT_DIR\n",
        "\n",
        "if not (Path(p) / 'tmp').exists():\n",
        "    (Path(p) / 'tmp').mkdir()\n",
        "\n",
        "img1.save(str(Path(p) / 'tmp' / 'img1.png'))\n",
        "img2.save(str(Path(p) / 'tmp' / 'img2.png'))\n",
        "\n",
        "%cd $p\n",
        "!zip -r \"tmp.zip\" \"tmp\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MykrBSWNQQlq"
      },
      "source": [
        "Распакуем архив tmp.zip в директорию tmp2 в PROJECT_DIR. Теперь внутри директории tmp2 содержится директория tmp, внутри которой находятся 2 изображения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CwSWrYIWMAus"
      },
      "outputs": [],
      "source": [
        "p = \"/content/drive/MyDrive/\" + PROJECT_DIR\n",
        "%cd $p\n",
        "!unzip -uq \"tmp.zip\" -d \"tmp2\""
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "7af69d84c46e0da4f71f361435e72c01e713b5d1fcbc89c051c042527a934273"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "15816baa917449adab89fb07c21f87e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d0331656f634461ebca86d9e1afc8a40",
              "IPY_MODEL_7e3fa845435143d88a8e2ebd970e26b6",
              "IPY_MODEL_12f2ff1faabe4b8f8911b58836494ccf"
            ],
            "layout": "IPY_MODEL_feb4f34181e943b8b964bc747859e8cc"
          }
        },
        "d0331656f634461ebca86d9e1afc8a40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17e3e8962808497fb926bbbaedb3e770",
            "placeholder": "​",
            "style": "IPY_MODEL_d20b7752b81d4b6a8583920760ed8a1d",
            "value": "100%"
          }
        },
        "7e3fa845435143d88a8e2ebd970e26b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a93b1ec91f474b9f89f7721cc3ec7cac",
            "max": 4500,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c114ae6f541e4a9793cf29b6d5abb05a",
            "value": 4500
          }
        },
        "12f2ff1faabe4b8f8911b58836494ccf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8259e21d75da4b23bdd325fb53ef2daf",
            "placeholder": "​",
            "style": "IPY_MODEL_bc37b4fd14b140e1bc03bcbfa372a535",
            "value": " 4500/4500 [00:27&lt;00:00, 167.80it/s]"
          }
        },
        "feb4f34181e943b8b964bc747859e8cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "17e3e8962808497fb926bbbaedb3e770": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d20b7752b81d4b6a8583920760ed8a1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a93b1ec91f474b9f89f7721cc3ec7cac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c114ae6f541e4a9793cf29b6d5abb05a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8259e21d75da4b23bdd325fb53ef2daf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc37b4fd14b140e1bc03bcbfa372a535": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f94d8f85465481c988049e377e0ac13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f08f09d33f04457b3d56f4d71a261ba",
              "IPY_MODEL_c3f8fad1c317420890fbb6e6bbbb263b",
              "IPY_MODEL_22e51226d4a2426eb5b36c63b0b31bb1"
            ],
            "layout": "IPY_MODEL_437a5fc962a045869987341f37b4f680"
          }
        },
        "3f08f09d33f04457b3d56f4d71a261ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d9d73a5dcbd48c4aa1c9b2e5bdbf99f",
            "placeholder": "​",
            "style": "IPY_MODEL_d7d8bec96dc942089c726cb6a72ffc58",
            "value": "100%"
          }
        },
        "c3f8fad1c317420890fbb6e6bbbb263b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8c39009b9ae46058c2f4caa142fd449",
            "max": 90,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_24e4bac929c745c09b205aee2b1a51ae",
            "value": 90
          }
        },
        "22e51226d4a2426eb5b36c63b0b31bb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_004c3e6d76134a1ba1621ee63d4efaf2",
            "placeholder": "​",
            "style": "IPY_MODEL_45dd2ae0954149989e92600fc45c4434",
            "value": " 90/90 [00:00&lt;00:00, 115.41it/s]"
          }
        },
        "437a5fc962a045869987341f37b4f680": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d9d73a5dcbd48c4aa1c9b2e5bdbf99f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7d8bec96dc942089c726cb6a72ffc58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a8c39009b9ae46058c2f4caa142fd449": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "24e4bac929c745c09b205aee2b1a51ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "004c3e6d76134a1ba1621ee63d4efaf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45dd2ae0954149989e92600fc45c4434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}